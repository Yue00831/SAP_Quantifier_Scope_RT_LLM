<!DOCTYPE html>
<!-- saved from url=(0060)file:///Users/liyue/Downloads/quantifier_scope_pipeline.html -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Quantifier Scope Ambiguity Pipeline - Multiple LLMs &amp; Bilingual Design</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', 'Arial', sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 30px 20px;
            min-height: 100vh;
        }
        
        .container {
            max-width: 1600px;
            margin: 0 auto;
            background: white;
            border-radius: 15px;
            padding: 50px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
        }
        
        /* Header */
        .header {
            text-align: center;
            margin-bottom: 50px;
            padding-bottom: 30px;
            border-bottom: 4px solid #667eea;
        }
        
        .main-title {
            color: #2d3748;
            font-size: 2.5em;
            font-weight: bold;
            margin-bottom: 15px;
            line-height: 1.2;
        }
        
        .subtitle {
            color: #718096;
            font-size: 1.2em;
            margin: 8px 0;
            line-height: 1.5;
        }
        
        .highlight {
            color: #667eea;
            font-weight: bold;
        }
        
        .subtitle.special {
            color: #e91e63;
            font-style: italic;
            margin-top: 15px;
        }
        
        /* Language Tracks */
        .language-section {
            margin: 40px 0 60px 0;
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 40px;
        }
        
        .language-track {
            background: #f8f9fa;
            border-radius: 12px;
            padding: 25px;
            border: 3px solid #dee2e6;
        }
        
        .language-title {
            font-size: 1.5em;
            font-weight: bold;
            margin-bottom: 20px;
            text-align: center;
            color: #2d3748;
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 10px;
        }
        
        .flag {
            font-size: 1.5em;
        }
        
        .stimuli-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin-top: 20px;
        }
        
        .stimuli-box {
            padding: 20px;
            border-radius: 8px;
            border: 2px solid;
        }
        
        .stimuli-box-title {
            font-weight: bold;
            font-size: 1.1em;
            margin-bottom: 10px;
            text-align: center;
        }
        
        .stimuli-box-content {
            font-size: 0.9em;
            line-height: 1.6;
        }
        
        .experimental {
            background: #e3f2fd;
            border-color: #2196f3;
        }
        
        .experimental .stimuli-box-title {
            color: #1565c0;
        }
        
        .filler {
            background: #fff3e0;
            border-color: #ff9800;
        }
        
        .filler .stimuli-box-title {
            color: #e65100;
        }
        
        .example-sentence {
            font-family: 'Courier New', monospace;
            background: rgba(255,255,255,0.7);
            padding: 8px;
            border-radius: 4px;
            margin: 8px 0;
            font-size: 0.85em;
        }
        
        .highlight-word {
            font-weight: bold;
            text-decoration: underline;
        }
        
        /* Multiple LLMs Section */
        .llm-section {
            margin: 60px 0;
            text-align: center;
        }
        
        .llm-title {
            font-size: 1.8em;
            font-weight: bold;
            color: #2d3748;
            margin-bottom: 30px;
        }
        
        .llm-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 25px;
            margin-top: 30px;
        }
        
        .llm-card {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px 20px;
            border-radius: 12px;
            box-shadow: 0 8px 20px rgba(0,0,0,0.15);
            transition: transform 0.3s ease;
        }
        
        .llm-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 12px 30px rgba(0,0,0,0.25);
        }
        
        .llm-icon {
            font-size: 3em;
            margin-bottom: 15px;
        }
        
        .llm-name {
            font-size: 1.3em;
            font-weight: bold;
            margin-bottom: 8px;
        }
        
        .llm-description {
            font-size: 0.9em;
            opacity: 0.9;
        }
        
        /* Arrow */
        .arrow-down {
            text-align: center;
            font-size: 3em;
            color: #667eea;
            margin: 30px 0;
        }
        
        /* Pipeline Flow */
        .pipeline-section {
            margin-top: 60px;
        }
        
        .pipeline-title {
            font-size: 2em;
            font-weight: bold;
            color: #2d3748;
            text-align: center;
            margin-bottom: 40px;
        }
        
        .pipeline-row {
            display: grid;
            grid-template-columns: 1fr auto 1fr auto 1fr;
            gap: 20px;
            align-items: stretch;
            margin: 40px 0;
        }
        
        .pipeline-box {
            background: white;
            padding: 30px;
            border-radius: 12px;
            border: 3px solid;
            min-height: 250px;
            display: flex;
            flex-direction: column;
            justify-content: center;
            box-shadow: 0 4px 12px rgba(0,0,0,0.1);
        }
        
        .box-header {
            font-size: 1.2em;
            font-weight: bold;
            margin-bottom: 15px;
            text-align: center;
            padding-bottom: 10px;
            border-bottom: 2px solid;
        }
        
        .box-content {
            font-size: 0.95em;
            line-height: 1.7;
        }
        
        .box-content ul {
            margin: 10px 0;
            padding-left: 20px;
        }
        
        .box-content li {
            margin: 8px 0;
        }
        
        .box-orange {
            border-color: #ff9800;
            background: #fff8f0;
        }
        
        .box-orange .box-header {
            color: #e65100;
            border-color: #ff9800;
        }
        
        .box-pink {
            border-color: #e91e63;
            background: #fef5f9;
        }
        
        .box-pink .box-header {
            color: #c2185b;
            border-color: #e91e63;
        }
        
        .box-blue {
            border-color: #2196f3;
            background: #f0f8ff;
        }
        
        .box-blue .box-header {
            color: #1565c0;
            border-color: #2196f3;
        }
        
        .box-green {
            border-color: #4caf50;
            background: #f1f8f4;
        }
        
        .box-green .box-header {
            color: #2e7d32;
            border-color: #4caf50;
        }
        
        .arrow-right {
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 2.5em;
            color: #667eea;
            font-weight: bold;
        }
        
        .formula-box {
            background: #e8f4f8;
            padding: 12px;
            border-radius: 6px;
            font-family: 'Courier New', monospace;
            font-size: 0.85em;
            margin: 15px 0;
            border-left: 4px solid #2196f3;
        }
        
        /* Results Section */
        .results-section {
            margin-top: 60px;
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            padding: 40px;
            border-radius: 12px;
        }
        
        .results-title {
            font-size: 2em;
            font-weight: bold;
            color: #2d3748;
            text-align: center;
            margin-bottom: 30px;
        }
        
        .results-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 30px;
        }
        
        .result-card {
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 4px 15px rgba(0,0,0,0.1);
        }
        
        .result-card-title {
            font-size: 1.3em;
            font-weight: bold;
            color: #2d3748;
            margin-bottom: 15px;
            display: flex;
            align-items: center;
            gap: 10px;
        }
        
        .result-icon {
            font-size: 1.5em;
        }
        
        .result-content {
            color: #4a5568;
            line-height: 1.8;
        }
        
        /* Key Features */
        .features-section {
            margin-top: 60px;
            background: #fff8dc;
            padding: 40px;
            border-radius: 12px;
            border: 3px solid #ffd700;
        }
        
        .features-title {
            font-size: 1.8em;
            font-weight: bold;
            color: #856404;
            text-align: center;
            margin-bottom: 30px;
        }
        
        .features-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 25px;
        }
        
        .feature-card {
            background: rgba(255,255,255,0.8);
            padding: 20px;
            border-radius: 8px;
            border-left: 5px solid #ff9800;
        }
        
        .feature-title {
            font-weight: bold;
            font-size: 1.1em;
            color: #2d3748;
            margin-bottom: 10px;
        }
        
        .feature-content {
            color: #4a5568;
            line-height: 1.6;
            font-size: 0.95em;
        }
        
        /* Info badges */
        .badge {
            display: inline-block;
            padding: 4px 12px;
            border-radius: 12px;
            font-size: 0.85em;
            font-weight: bold;
            margin: 5px 5px 5px 0;
        }
        
        .badge-blue {
            background: #e3f2fd;
            color: #1565c0;
        }
        
        .badge-orange {
            background: #fff3e0;
            color: #e65100;
        }
        
        .badge-green {
            background: #e8f5e9;
            color: #2e7d32;
        }
        
        .badge-purple {
            background: #f3e5f5;
            color: #6a1b9a;
        }
    </style>
</head>
<body>
    <div class="container">
        <!-- Header -->
        <div class="header">
            <div class="main-title">
                Quantifier Scope Ambiguity Human RT &gt;&lt; LLM Pipeline
            </div>
            <div class="subtitle">
                Testing <span class="highlight">Multiple Language Models</span> on Bilingual Psycholinguistic Data
            </div>
            <div class="subtitle">
                Experimental design: Quantifier scope sentences (Ambiguous) vs. Unambiguous Structures (Fillers)
            </div>
            <div class="subtitle special">
                Goal: Determine if LLM surprisal predicts human reading difficulty for quantifier scope ambiguities
            </div>
        </div>
        
        <!-- Language Tracks -->
        <div class="language-section">
            <!-- English Track -->
            <div class="language-track">
                <div class="language-title">
                    <span class="flag">üß†</span>
                    English Stimuli
                </div>
                
                <div class="stimuli-grid">
                    <div class="stimuli-box experimental">
                        <div class="stimuli-box-title">
                            Experimental items
                        </div>
                        <div class="stimuli-box-content">
                            <strong>Type:</strong> Quantifier scope ambiguity<br>
                            <strong>n =</strong> 48 sentences<br><br>
                            <div class="example-sentence">
                                A boy fed every dog, although the boy was not very interested. </div>
                            <small style="color: #666;">
                                <strong>We collect:</strong>Reaction time (word-by-word)</small>
                        </div>
                    </div>
                    
                    <div class="stimuli-box filler">
                        <div class="stimuli-box-title">
                            Filler items
                        </div>
                        <div class="stimuli-box-content">
                            <strong>Type:</strong> Huang et al., (2024)'s critical items<br>
                            <strong>4 Structure Types:</strong><br>
                            <div style="margin-top: 8px;">
                                <span class="badge badge-orange">relative clause</span>
                                <span class="badge badge-orange">agreement error</span>
                                <span class="badge badge-orange">attachment</span>
                                <span class="badge badge-orange">garden path</span>
                            </div>
                            <div class="example-sentence" style="margin-top: 10px;">
                                The girl fed the lamb <span class="highlight-word">remained</span> calm...
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            
            <!-- Chinese Track -->
            <div class="language-track">
                <div class="language-title">
                    <span class="flag">üß†</span>
                    Chinese Stimuli (‰∏≠Êñá)
                </div>
                
                <div class="stimuli-grid">
                    <div class="stimuli-box experimental">
                        <div class="stimuli-box-title">
                            Experimental items
                        </div>
                        <div class="stimuli-box-content">
                            <strong>Type:</strong> Direct Translation<br>
                            <strong>n =</strong> 48 sentences<br><br>
                            <div class="example-sentence">
                               ‰∏Ä‰∏™Áî∑Â≠©ÂñÇ‰∫ÜÊØè‰∏ÄÂè™ÁãóÔºåËôΩÁÑ∂Ëøô‰∏™Áî∑Â≠©‰∏çÊÑüÂÖ¥Ë∂£„ÄÇ</div>
                            <small style="color: #666;">
                                Translation of English quantifier scope sentences
                            </small>
                        </div>
                    </div>
                    
                    <div class="stimuli-box filler">
                        <div class="stimuli-box-title">
                            Filler items
                        </div>
                        <div class="stimuli-box-content">
                            <strong>Type:</strong> Different structures<br>
                            <strong>Source:</strong> several previous studies<br><br>
                            <div class="example-sentence">
                                ‰∏≠ÊñáÂ§çÊùÇÂè•
                            </div>
                            <small style="color: #666;">
                                Various syntactic structures from Chinese psycholinguistic literature
                            </small>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arrow-down">‚Üì</div>
        
        <!-- Multiple LLMs Section -->
        <div class="llm-section">
            <div class="llm-title">
                ü§ñ Multiple LMs Evaluation
            </div>
            <p style="color: #718096; font-size: 1.1em; margin-bottom: 30px;">
                Comparative evaluation across different model architectures and sizes
            </p>
            
            <div class="llm-grid">
                <div class="llm-card">
                    <div class="llm-icon">1Ô∏è‚É£</div>
                    <div class="llm-name">GPT-2 Small</div>
                    <div class="llm-description">124M parameters<br>Baseline model</div>
                </div>
                
                <div class="llm-card">
                    <div class="llm-icon">2Ô∏è‚É£</div>
                    <div class="llm-name">GPT-2 Medium</div>
                    <div class="llm-description">355M parameters<br>Mid-size comparison</div>
                </div>
                
                <div class="llm-card">
                    <div class="llm-icon">3Ô∏è‚É£</div>
                    <div class="llm-name">GPT-2 Large</div>
                    <div class="llm-description">774M parameters<br>Larger capacity</div>
                </div>
                
                <div class="llm-card">
                    <div class="llm-icon">4Ô∏è‚É£</div>
                    <div class="llm-name">BERT</div>
                    <div class="llm-description">110M/340M<br>Bidirectional context</div>
                </div>
                
                <div class="llm-card">
                    <div class="llm-icon">5Ô∏è‚É£</div>
                    <div class="llm-name">LLaMA-2</div>
                    <div class="llm-description">7B/13B parameters<br>Modern architecture</div>
                </div>
                
                <div class="llm-card">
                    <div class="llm-icon">‚ö°</div>
                    <div class="llm-name">Our choice</div>
                    <div class="llm-description">Extensible design<br>LMs with different features</div>
                </div>
            </div>
            
            <p style="color: #e91e63; font-weight: bold; margin-top: 30px; font-size: 1.1em;">
                Each LM generates word-by-word surprisal: Surprisal = -log‚ÇÇ P(word|context)
            </p>
        </div>
        
        <div class="arrow-down">‚Üì</div>
        
        <!-- Pipeline Flow -->
        <div class="pipeline-section">
            <div class="pipeline-title">
                Analysis pipeline: following Huang et al.'s approach
            </div>
            
            <div class="pipeline-row">
                <!-- Step 1 -->
                <div class="pipeline-box box-orange">
                    <div class="box-header">
                        Step 1: Extract Predictors (using Fillers)<br>
                        <small style="font-size: 0.7em; font-weight: normal;">Goal: establish a linear relationship between surprisal and human RT</small>
                        <small style="font-size: 0.7em; font-weight: normal;">Schijndel and Linzen (2021)</small>
                    </div>
                    <div class="box-content">
                        <strong>For each word in filler items:</strong>
                        <ul>
                            <li><strong>Surprisal</strong> (w‚ÇÄ, w‚Çã‚ÇÅ, w‚Çã‚ÇÇ, w‚Çã‚ÇÉ)</li>
                            <li><strong>Position</strong> in sentence</li>
                            <li><strong>Word length</strong></li>
                            <li><strong>Log frequency</strong></li>
                            <li><strong>Freq √ó Length</strong></li>
                        </ul>
                        <span class="badge badge-orange">Spillover: 3 words back</span><br>
                        <span class="badge badge-orange">Exclude: First 3 &amp; last word (Smith and Levy, 2013)</span>
                        <span class="badge badge-orange">All predictors are centered and scaled</span>
                    </div>
                </div>
                
                <div class="arrow-right">‚Üí</div>
                
                <!-- Step 2 -->
                <div class="pipeline-box box-pink">
                    <div class="box-header">
                        Step 2: Fit Linking Function
                    </div>
                    <div class="box-content">
                        <strong>Mixed-effects model on FILLERS:</strong>
                        <div class="formula-box">
RT ~ surprisal + position + 
    length + frequency + 
    freq√ólength +
    spillover(w‚Çã‚ÇÅ, w‚Çã‚ÇÇ, w‚Çã‚ÇÉ) +
    (1|participant) + 
    (1|item) +
    (surprisal|participant)
                        </div>
                        <span class="badge badge-pink">Establish surprisal‚ÜíRT mapping</span><br>
                        <strong>Deliverables</strong><span class="badge badge-pink">a set of 'conversion factors"</span>
                    </div>
                </div>
                
                <div class="arrow-right">‚Üí</div>
                
                <!-- Step 3 -->
                <div class="pipeline-box box-blue">
                    <div class="box-header">
                        Step 3: Generate Predictions<br>
                        <small style="font-size: 0.7em; font-weight: normal;">(Experimental Items)</small>
                    </div>
                    <div class="box-content">
                        <strong>Apply linking function:</strong>
                        <ul>
                            <li>Use 'conversion factors' derived from Step 2</li>
                            <li>Predict RT for each word in our quantifier scope sentences (experimental items)</li>
                            <li>Generate predictions for EACH LM</li>
                        </ul>
                        <div style="margin-top: 15px;">
                            <span class="badge badge-blue">GPT-2 Small predictions</span><br>
                            <span class="badge badge-blue">GPT-2 Medium predictions</span><br>
                            <span class="badge badge-blue">+ All other LMs...</span>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="arrow-down">‚Üì</div>
            
            <div class="pipeline-row" style="grid-template-columns: 1fr;">
                <!-- Step 4 -->
                <div class="pipeline-box box-green">
                    <div class="box-header">
                        Step 4: Compare and evaluate
                    </div>
                    <div class="box-content">
                        <strong>Bayesian regression analysis:</strong>
                        <ul>
                            <li><strong>Observed:</strong> Human reading times for quantifier scope sentences</li>
                            <li><strong>Predicted:</strong> LM-derived RT predictions (from each model)</li>
                            <li><strong>Analysis:</strong> Which LM best captures the ambiguity processing effect?</li>
                        </ul>
                        
                        <div style="margin-top: 20px;">
                            <strong>Key comparisons:</strong><br>
                            <span class="badge badge-green">Effect size alignment</span>
                            <span class="badge badge-green">Correlation strength</span>
                            <span class="badge badge-green">Cross-model comparison</span>
                            <span class="badge badge-purple">Cross-language comparison</span>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <!-- Results Section -->
        <div class="results-section">
            <div class="results-title">
                What we expect to get/find
            </div>
            
            <div class="results-grid">
                <div class="result-card">
                    <div class="result-card-title">
                        <span class="result-icon"></span>
                        Model performance
                    </div>
                    <div class="result-content">
                        <strong>For each language model:</strong>
                        <ul style="margin-top: 10px;">
                            <li>Correlation between predicted and observed RTs</li>
                            <li>Effect size accuracy at ambiguous regions</li>
                            <li>Spillover pattern alignment</li>
                        </ul>
                        <div style="margin-top: 15px; padding: 10px; background: #f0f8ff; border-radius: 6px;">
                            <strong>Question:</strong> Which model architecture best predicts quantifier scope processing?
                        </div>
                    </div>
                </div>
                
                <div class="result-card">
                    <div class="result-card-title">
                        <span class="result-icon"></span>
                        Cross-linguistic patterns
                    </div>
                    <div class="result-content">
                        <strong>English vs. Chinese:</strong>
                        <ul style="margin-top: 10px;">
                            <li>Do models show similar prediction accuracy across languages?</li>
                            <li>Are there language-specific processing patterns?</li>
                            <li>Do Chinese models perform better on Chinese data?</li>
                        </ul>
                        <div style="margin-top: 15px; padding: 10px; background: #fff3e0; border-radius: 6px;">
                            <strong>Question:</strong> Is quantifier scope processing universal or language-specific?
                        </div>
                    </div>
                </div>
                
                <div class="result-card">
                    <div class="result-card-title">
                        <span class="result-icon"></span>
                        Theoretical implications
                    </div>
                    <div class="result-content">
                        <strong>Key insights:</strong>
                        <ul style="margin-top: 10px;">
                            <li>Does surprisal capture semantic ambiguity processing?</li>
                            <li>How does it differ from syntactic garden paths (fillers)?</li>
                            <li>Role of model size and architecture</li>
                        </ul>
                        <div style="margin-top: 15px; padding: 10px; background: #f1f8f4; border-radius: 6px;">
                            <strong>Impact:</strong> Bridges computational linguistics and psycholinguistics
                        </div>
                    </div>
                </div>
                
                <div class="result-card">
                    <div class="result-card-title">
                        <span class="result-icon"></span>
                        Methodological innovation
                    </div>
                    <div class="result-content">
                        <strong>Novel contributions:</strong>
                        <ul style="margin-top: 10px;">
                            <li>First multi-LM comparison on quantifier scope</li>
                            <li>Bilingual parallel design (EN/ZH)</li>
                            <li>Replication of Huang's linking function approach</li>
                        </ul>
                        <div style="margin-top: 15px; padding: 10px; background: #fef5f9; border-radius: 6px;">
                            <strong>Contribution:</strong> Benchmark dataset for semantic ambiguity
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <!-- Key Features -->
        <div class="features-section">
            <div class="features-title">
                Key designs
            </div>
            
            <div class="features-grid">
                <div class="feature-card">
                    <div class="feature-title">Role reversal design</div>
                    <div class="feature-content">
                        Unlike Huang et al., our quantifier scope sentences are experimental items, while their garden path structures (along with others) serve as fillers. This tests whether linking functions generalize across phenomenon types.
                    </div>
                </div>
                
                <div class="feature-card">
                    <div class="feature-title">Bilingual parallel design</div>
                    <div class="feature-content">
                        Direct translations allow cross-linguistic comparison. Chinese fillers use different structures to maintain naturalness while establishing baseline processing patterns.
                    </div>
                </div>
                
                <div class="feature-card">
                    <div class="feature-title">Multi-model architecture</div>
                    <div class="feature-content">
                        Testing multiple LMs (GPT-2 variants, BERT, LLaMA, etc.) reveals whether surprisal effects are model-dependent or represent general properties of language processing.
                    </div>
                </div>
                
                <div class="feature-card">
                    <div class="feature-title">Spillover effects</div>
                    <div class="feature-content">
                        Including w‚Çã‚ÇÅ, w‚Çã‚ÇÇ, w‚Çã‚ÇÉ predictors captures delayed processing effects critical for understanding how ambiguity resolution unfolds over time.
                    </div>
                </div>
                
                <div class="feature-card">
                    <div class="feature-title">Semantic ambiguity focus</div>
                    <div class="feature-content">
                        Quantifier scope ambiguities are semantic (not purely syntactic), testing whether LM surprisal extends beyond syntactic features to semantic-pragmatic processing.
                    </div>
                </div>
                
                <div class="feature-card">
                    <div class="feature-title">Replicable framework</div>
                    <div class="feature-content">
                        Following Huang's established methodology gives us chances to compare, while also extending to new phenomena and languages.
                    </div>
                </div>
            </div>
        </div>
        
        <!-- Footer -->
        <div style="margin-top: 60px; text-align: center; padding: 30px; background: #f8f9fa; border-radius: 10px;">
            <p style="color: #4a5568; font-size: 1.1em; line-height: 1.8;">
                <strong>Pipeline summary:</strong> This design tests whether language model surprisal can predict human reading difficulty for <em>quantifier scope ambiguities</em> across multiple models and languages, using Huang et al (2024).'s validated linking function approach with classic garden path structures as neutral baseline items.
            </p>
        </div>
    </div>


</body></html>
